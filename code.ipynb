{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa9b0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43848afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py\n",
    "\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=8,\n",
    "        )\n",
    "    # concatenate all the ids in each dataset into one large file we can use for training\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 1024\n",
    "\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "            # Batch together samples for faster write\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "            # Write into mmap\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b96bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\n",
    "#block size = context window\n",
    "def get_batch(split):\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff3b83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c6abfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=50257,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7146a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd17a6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x294e1331cd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Config\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "learning_rate = 1e-4 #more stable training, earlier 1e-4\n",
    "max_iters = 20000 #increase from 25000\n",
    "warmup_steps = 1000 #smoother initial train, earlier 100\n",
    "min_lr = 5e-4 #lower rate, earlier 5e-4\n",
    "eval_iters = 500 # increased from 100\n",
    "batch_size = 32 # changed from 16, better gradient estimate\n",
    "block_size = 128 #changed from 64, capture longer range dependencies\n",
    "\n",
    "gradient_accumulation_steps = 32 # reduced from 50\n",
    "\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "\n",
    "# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n",
    "#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e022fb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush\\AppData\\Local\\Temp\\ipykernel_16012\\2132813893.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
    "\n",
    "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
    "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
    "\n",
    "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b09ad4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b41805493a4e9e8a7d47ad602b57d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITMStudy\\MajorProjects\\My_own_SLM\\venv313\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: train loss 9.4933, val loss 9.5003\n",
      "The current learning rate: 0.00007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IITMStudy\\MajorProjects\\My_own_SLM\\venv313\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: train loss 8.5269, val loss 8.5306\n",
      "The current learning rate: 0.00010\n",
      "Epoch 1500: train loss 7.5852, val loss 7.5870\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2000: train loss 6.7341, val loss 6.7359\n",
      "The current learning rate: 0.00010\n",
      "Epoch 2500: train loss 6.0322, val loss 6.0303\n",
      "The current learning rate: 0.00011\n",
      "Epoch 3000: train loss 5.5215, val loss 5.5185\n",
      "The current learning rate: 0.00011\n",
      "Epoch 3500: train loss 5.0918, val loss 5.0842\n",
      "The current learning rate: 0.00012\n",
      "Epoch 4000: train loss 4.7534, val loss 4.7516\n",
      "The current learning rate: 0.00012\n",
      "Epoch 4500: train loss 4.5043, val loss 4.5055\n",
      "The current learning rate: 0.00013\n",
      "Epoch 5000: train loss 4.2845, val loss 4.2823\n",
      "The current learning rate: 0.00014\n",
      "Epoch 5500: train loss 4.1109, val loss 4.1172\n",
      "The current learning rate: 0.00015\n",
      "Epoch 6000: train loss 3.9494, val loss 3.9534\n",
      "The current learning rate: 0.00016\n",
      "Epoch 6500: train loss 3.8107, val loss 3.8135\n",
      "The current learning rate: 0.00018\n",
      "Epoch 7000: train loss 3.7079, val loss 3.7086\n",
      "The current learning rate: 0.00019\n",
      "Epoch 7500: train loss 3.5939, val loss 3.5950\n",
      "The current learning rate: 0.00020\n",
      "Epoch 8000: train loss 3.4976, val loss 3.5031\n",
      "The current learning rate: 0.00022\n",
      "Epoch 8500: train loss 3.4068, val loss 3.4147\n",
      "The current learning rate: 0.00024\n",
      "Epoch 9000: train loss 3.3191, val loss 3.3250\n",
      "The current learning rate: 0.00025\n",
      "Epoch 9500: train loss 3.2489, val loss 3.2541\n",
      "The current learning rate: 0.00027\n",
      "Epoch 10000: train loss 3.1746, val loss 3.1810\n",
      "The current learning rate: 0.00028\n",
      "Epoch 10500: train loss 3.1014, val loss 3.1088\n",
      "The current learning rate: 0.00030\n",
      "Epoch 11000: train loss 3.0529, val loss 3.0574\n",
      "The current learning rate: 0.00032\n",
      "Epoch 11500: train loss 2.9985, val loss 2.9943\n",
      "The current learning rate: 0.00033\n",
      "Epoch 12000: train loss 2.9357, val loss 2.9368\n",
      "The current learning rate: 0.00035\n",
      "Epoch 12500: train loss 2.8801, val loss 2.8863\n",
      "The current learning rate: 0.00036\n",
      "Epoch 13000: train loss 2.8403, val loss 2.8460\n",
      "The current learning rate: 0.00038\n",
      "Epoch 13500: train loss 2.7921, val loss 2.7949\n",
      "The current learning rate: 0.00040\n",
      "Epoch 14000: train loss 2.7427, val loss 2.7480\n",
      "The current learning rate: 0.00041\n",
      "Epoch 14500: train loss 2.7102, val loss 2.7105\n",
      "The current learning rate: 0.00042\n",
      "Epoch 15000: train loss 2.6640, val loss 2.6651\n",
      "The current learning rate: 0.00044\n",
      "Epoch 15500: train loss 2.6336, val loss 2.6310\n",
      "The current learning rate: 0.00045\n",
      "Epoch 16000: train loss 2.5933, val loss 2.5987\n",
      "The current learning rate: 0.00046\n",
      "Epoch 16500: train loss 2.5720, val loss 2.5771\n",
      "The current learning rate: 0.00047\n",
      "Epoch 17000: train loss 2.5216, val loss 2.5245\n",
      "The current learning rate: 0.00048\n",
      "Epoch 17500: train loss 2.5041, val loss 2.5006\n",
      "The current learning rate: 0.00048\n",
      "Epoch 18000: train loss 2.4706, val loss 2.4767\n",
      "The current learning rate: 0.00049\n",
      "Epoch 18500: train loss 2.4345, val loss 2.4430\n",
      "The current learning rate: 0.00049\n",
      "Epoch 19000: train loss 2.4053, val loss 2.4204\n",
      "The current learning rate: 0.00050\n",
      "Epoch 19500: train loss 2.3891, val loss 2.3910\n",
      "The current learning rate: 0.00050\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# In your training loop\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        # Ensure estimate_loss uses the correct device\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list += [losses['train']]\n",
    "        validation_loss_list += [losses['val']]\n",
    "\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    # Ensure X and y are on the correct device\n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2477f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Device type: cpu\n",
      "Data type: float16\n",
      "CUDA available: False\n",
      "Model parameters: 29,995,392\n",
      "Model size: 120.0 MB (float32)\n",
      "train.bin: 943.7 MB\n",
      "val.bin: NOT FOUND\n"
     ]
    }
   ],
   "source": [
    "# # Check training setup and performance\n",
    "# print(f\"Device: {device}\")\n",
    "# print(f\"Device type: {device_type}\")\n",
    "# print(f\"Data type: {dtype}\")\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "#     print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# # Check model size\n",
    "# param_count = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"Model parameters: {param_count:,}\")\n",
    "# print(f\"Model size: {param_count * 4 / 1e6:.1f} MB (float32)\")\n",
    "\n",
    "# # Check if binary files exist\n",
    "# import os\n",
    "# files_to_check = ['train.bin', 'val.bin']\n",
    "# for file in files_to_check:\n",
    "#     if os.path.exists(file):\n",
    "#         size = os.path.getsize(file) / 1e6\n",
    "#         print(f\"{file}: {size:.1f} MB\")\n",
    "#     else:\n",
    "#         print(f\"{file}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b4444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU Diagnostics ===\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA compiled version: 11.8\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "cuDNN version: 90100\n",
      "cuDNN enabled: True\n",
      "nvidia-smi output:\n",
      "Thu Dec  4 18:16:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.97                 Driver Version: 580.97         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|       ...\n",
      "nvcc not available: [WinError 2] The system cannot find the file specified\n",
      "=== Recommendations ===\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA compiled version: 11.8\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "cuDNN version: 90100\n",
      "cuDNN enabled: True\n",
      "nvidia-smi output:\n",
      "Thu Dec  4 18:16:46 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.97                 Driver Version: 580.97         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|       ...\n",
      "nvcc not available: [WinError 2] The system cannot find the file specified\n",
      "=== Recommendations ===\n"
     ]
    }
   ],
   "source": [
    "# # Detailed GPU diagnostics\n",
    "# print(\"=== GPU Diagnostics ===\")\n",
    "\n",
    "# # Check PyTorch installation and CUDA support\n",
    "# import torch\n",
    "# print(f\"PyTorch version: {torch.__version__}\")\n",
    "# print(f\"CUDA compiled version: {torch.version.cuda}\")\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "# print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "# # Check if PyTorch was installed with CUDA support\n",
    "# try:\n",
    "#     print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "#     print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")\n",
    "# except:\n",
    "#     print(\"cuDNN: Not available\")\n",
    "\n",
    "# # Check NVIDIA driver and CUDA installation\n",
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# try:\n",
    "#     # Check nvidia-smi\n",
    "#     result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)\n",
    "#     if result.returncode == 0:\n",
    "#         print(\"nvidia-smi output:\")\n",
    "#         print(result.stdout[:500] + \"...\" if len(result.stdout) > 500 else result.stdout)\n",
    "#     else:\n",
    "#         print(\"nvidia-smi failed or not found\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error running nvidia-smi: {e}\")\n",
    "\n",
    "# # Check CUDA runtime version if available\n",
    "# try:\n",
    "#     result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True, timeout=10)\n",
    "#     if result.returncode == 0:\n",
    "#         print(\"CUDA Compiler (nvcc) version:\")\n",
    "#         print(result.stdout)\n",
    "#     else:\n",
    "#         print(\"nvcc not found\")\n",
    "# except Exception as e:\n",
    "#     print(f\"nvcc not available: {e}\")\n",
    "\n",
    "# print(\"=== Recommendations ===\")\n",
    "# if not torch.cuda.is_available():\n",
    "#     print(\"ISSUE: PyTorch cannot access CUDA\")\n",
    "#     print(\"Possible solutions:\")\n",
    "#     print(\"1. Install PyTorch with CUDA support:\")\n",
    "#     print(\"   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "#     print(\"2. Make sure NVIDIA GPU drivers are installed\")\n",
    "#     print(\"3. Make sure CUDA toolkit is installed\")\n",
    "#     print(\"4. Restart the kernel after installing CUDA-enabled PyTorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30d3cdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model\n",
    "model = GPT(config)  # re-create the model with same config\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a8f02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a pumpkin. The ant and the seed were sad, but the boy wanted to make some food. But, the potato wanted to be too soft. So, the monkey was happy to eat the food. He chased his juicy leaf and they flew off the food.\n",
      "\n",
      "The strawberry stayed for a long time, happy and happy that the squash was not worth any truth. He was happy to be thankful to the pumpkin and keep it safe.\n",
      "\n",
      "The little duck spent a rest of this time getting more castles forever. Even matter what many days he was sleeping on. Its family loved the delicious fruit and decided to be letting it find.\n",
      "\n",
      "The animals made sure to shut the shell, taking the mushroom home, when the water could act. Everyone was so happy they hugged and the tomato and thanked the dough of that line of making us feel very happy.Mama always decided to eat a photo in the attic. It was a big surprise full of the oven. Sam was a beautiful picture of her\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Once upon a time there was a pumpkin.\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d7193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
